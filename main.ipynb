{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a613360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxpooling import maxPooling \n",
    "from softmax import softMax\n",
    "from Flatten import Flatten\n",
    "from Batchnorm import BatchNorm\n",
    "from Convolution_Function import Convolution \n",
    "from Dropout import Layer_Dropout \n",
    "from FullyConnectedLayer_Function import FullyConected\n",
    "from Convolution_Function import ReLU \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15a4537f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d7a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = Convolution(1, 8, 3) # 26x26x8 \n",
    "relu = ReLU()\n",
    "#batchnorm_layer = BatchNorm()\n",
    "pooling_layer = maxPooling(2, 2)  # 13x13x8 \n",
    "flatten_layer = Flatten()\n",
    "flc_layer1 = FullyConected(13*13*8, 128)\n",
    "dropout_layer1 = Layer_Dropout(0.5)\n",
    "flc_layer2 = FullyConected(128, 32)\n",
    "dropout_layer2 = Layer_Dropout(0.5)\n",
    "softmax_layer = softMax(32,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9102874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the CNN\n",
    "        X: input image of shape (height, width, channels)\n",
    "        \"\"\"\n",
    "        output = X\n",
    "        print(\"Original: \", output.shape)\n",
    "\n",
    "        # Convolutional layer\n",
    "        output = self.layers[0].forward(output)  # Convolution: (28, 28, 1) -> (26, 26, 8)\n",
    "        print(\"Conv layer:\", output.shape)\n",
    "\n",
    "        # ReLU activation\n",
    "        output = self.layers[1].forward(output)  # ReLU\n",
    "        print(\"ReLU layer: \", output.shape)\n",
    "\n",
    "        # Max pooling\n",
    "        output = self.layers[2].forward(output)  # MaxPooling: (26, 26, 8) -> (13, 13, 8)\n",
    "        print(\"Max pooling\", output.shape)\n",
    "\n",
    "        # Flatten\n",
    "        output = self.layers[3].forward(output)  # Flatten: (13, 13, 8) -> (1352, 1)\n",
    "        print(\"Flatten\", output.shape)\n",
    "        \n",
    "        # Fully connected layer 1\n",
    "        output = self.layers[4].forward(output)  # FC1: (1352,) -> (128,)\n",
    "        print(\"Fully 1: \", output.shape)\n",
    "        # ReLU activation\n",
    "        output = np.maximum(0, output)  # ReLU\n",
    "        print(\"ReLU: \", output.shape)\n",
    "        # Dropout layer 1\n",
    "        self.layers[5].forward(output)  # Dropout: (128,)\n",
    "        output = self.layers[5].output  # Apply dropout mask\n",
    "        print(\"Dropout: \", output.shape)\n",
    "        \n",
    "        # Fully connected layer 2\n",
    "        output = self.layers[6].forward(output)  # FC2: (128,) -> (32,)\n",
    "        \n",
    "        # ReLU activation\n",
    "        output = np.maximum(0, output)  # ReLU\n",
    "        \n",
    "        # Dropout layer 2\n",
    "        self.layers[7].forward(output)  # Dropout: (32,)\n",
    "        output = self.layers[7].output  # Apply dropout mask\n",
    "        \n",
    "        # Softmax\n",
    "        output = self.layers[8].forward(output)  # Softmax: (32,) -> (10,)\n",
    "        print(\"Softmax: \", output.shape)\n",
    "\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dY, learning_rate):\n",
    "        \"\"\"\n",
    "        Backward pass through the CNN\n",
    "        dY: gradient of loss with respect to softmax output\n",
    "        learning_rate: learning rate for weight updates\n",
    "        \"\"\"\n",
    "        # Softmax backward\n",
    "        dY = self.layers[8].backprop(dY, learning_rate)\n",
    "        print(\"Softmax backward\")\n",
    "\n",
    "        # Dropout layer 2 backward\n",
    "        dY = self.layers[7].backward(dY)\n",
    "        dY = np.maximum(0, dY)  # ReLU backward\n",
    "        \n",
    "        # FC2 backward\n",
    "        dY = self.layers[6].backprop(dY, learning_rate)\n",
    "        dY = np.maximum(0, dY)  # ReLU backward (simple version)\n",
    "        \n",
    "        # Dropout layer 1 backward\n",
    "        dY = self.layers[5].backward(dY)\n",
    "        dY = np.maximum(0, dY)  # ReLU backward\n",
    "        \n",
    "        # FC1 backward\n",
    "        dY = self.layers[4].backprop(dY, learning_rate)\n",
    "        \n",
    "        # Flatten backward\n",
    "        dY = self.layers[3].backward(dY)\n",
    "        \n",
    "        # MaxPooling backward\n",
    "        dY = self.layers[2].backprop(dY)\n",
    "        \n",
    "        # ReLU backward\n",
    "        dY = self.layers[1].backward(dY)\n",
    "        \n",
    "        # Convolution backward\n",
    "        dY = self.layers[0].backprop(dY, learning_rate)\n",
    "        \n",
    "        return dY\n",
    "    \n",
    "    def train(self, train_images, train_labels, epochs, batch_size, learning_rate):\n",
    "        \"\"\"\n",
    "        Train the CNN model\n",
    "        train_images: training images of shape (num_samples, 28, 28)\n",
    "        train_labels: training labels of shape (num_samples,)\n",
    "        epochs: number of training epochs\n",
    "        batch_size: batch size for training\n",
    "        learning_rate: learning rate for optimization\n",
    "        \"\"\"\n",
    "        num_samples = train_images.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            # Shuffle the data\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            train_images_shuffled = train_images[indices]\n",
    "            train_labels_shuffled = train_labels[indices]\n",
    "            \n",
    "            # Iterate through batches\n",
    "            for batch_idx in range(0, num_samples, batch_size):\n",
    "                end_idx = min(batch_idx + batch_size, num_samples)\n",
    "                batch_images = train_images_shuffled[batch_idx:end_idx]\n",
    "                batch_labels = train_labels_shuffled[batch_idx:end_idx]\n",
    "                \n",
    "                batch_loss = 0\n",
    "                \n",
    "                # Process each sample in the batch\n",
    "                for img, label in zip(batch_images, batch_labels):\n",
    "                    # Normalize image\n",
    "                    img_normalized = img.astype(np.float32) / 255.0\n",
    "                    img_normalized = img_normalized[np.newaxis, :, :]  # Add channel dimension\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    output = self.forward(img_normalized)\n",
    "                    \n",
    "                    # Compute cross-entropy loss\n",
    "                    output = np.clip(output, 1e-7, 1 - 1e-7)  # Avoid log(0)\n",
    "                    loss = -np.log(output[label])\n",
    "                    batch_loss += loss\n",
    "                    \n",
    "                    # Compute gradient of loss with respect to softmax output\n",
    "                    dY = np.zeros_like(output)\n",
    "                    dY[label] = -1 / output[label]\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    self.backward(dY, learning_rate)\n",
    "                \n",
    "                avg_batch_loss = batch_loss / (end_idx - batch_idx)\n",
    "                total_loss += avg_batch_loss\n",
    "                num_batches += 1\n",
    "                \n",
    "                if num_batches % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}, Batch {num_batches}: Loss = {avg_batch_loss:.4f}\")\n",
    "            \n",
    "            avg_epoch_loss = total_loss / num_batches\n",
    "            print(f\"Epoch {epoch+1} completed - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    def predict(self, test_images):\n",
    "        \"\"\"\n",
    "        Make predictions on test images\n",
    "        test_images: test images of shape (num_samples, 28, 28)\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for img in test_images:\n",
    "            # Normalize image\n",
    "            img_normalized = img.astype(np.float32) / 255.0\n",
    "            img_normalized = img_normalized[np.newaxis, :, :]  # Add channel dimension\n",
    "            \n",
    "            # Forward pass\n",
    "            output = self.forward(img_normalized)\n",
    "            \n",
    "            # Get predicted class\n",
    "            predicted_class = np.argmax(output)\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b808928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  (1, 28, 28)\n",
      "Conv layer: (8, 26, 26)\n",
      "ReLU layer:  (8, 26, 26)\n",
      "Max pooling (8, 13, 13)\n",
      "Flatten (1352,)\n",
      "Fully 1:  (1, 128)\n",
      "ReLU:  (1, 128)\n",
      "Dropout:  (1, 128)\n",
      "Softmax:  (10,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[31mTypeError\u001b[39m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      2\u001b[39m cnn_model = CNN([\n\u001b[32m      3\u001b[39m     conv_layer,        \u001b[38;5;66;03m# 0: Convolution\u001b[39;00m\n\u001b[32m      4\u001b[39m     relu,              \u001b[38;5;66;03m# 1: ReLU\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     softmax_layer      \u001b[38;5;66;03m# 9: Softmax\u001b[39;00m\n\u001b[32m     12\u001b[39m ])\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Train the model (example with small subset and few epochs for testing)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Uncomment to run:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mcnn_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Make predictions on test set\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# predictions = cnn_model.predict(test_images[:10])\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# print(\"Predictions:\", predictions)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mCNN.train\u001b[39m\u001b[34m(self, train_images, train_labels, epochs, batch_size, learning_rate)\u001b[39m\n\u001b[32m    140\u001b[39m     dY[label] = -\u001b[32m1\u001b[39m / output[label]\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m avg_batch_loss = batch_loss / (end_idx - batch_idx)\n\u001b[32m    146\u001b[39m total_loss += avg_batch_loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mCNN.backward\u001b[39m\u001b[34m(self, dY, learning_rate)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03mBackward pass through the CNN\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03mdY: gradient of loss with respect to softmax output\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03mlearning_rate: learning rate for weight updates\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Softmax backward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m dY = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Dropout layer 2 backward\u001b[39;00m\n\u001b[32m     67\u001b[39m dY = \u001b[38;5;28mself\u001b[39m.layers[\u001b[32m7\u001b[39m].backward(dY)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ZB\\Code\\Python\\CNN-from-strach\\softmax.py:35\u001b[39m, in \u001b[36msoftMax.backprop\u001b[39m\u001b[34m(self, dE_dY, alpha)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Compute gradients with respect to output (Z)\u001b[39;00m\n\u001b[32m     34\u001b[39m dY_dZ = -z_exp[i]*z_exp / (S**\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mdY_dZ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m = z_exp[i]*(S - z_exp) / (S**\u001b[32m2\u001b[39m)\n\u001b[32m     37\u001b[39m dE_dZ = gradient * dY_dZ \n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Gradients of totals against weights/biases/input\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Instantiate the CNN model\n",
    "cnn_model = CNN([\n",
    "    conv_layer,        # 0: Convolution\n",
    "    relu,              # 1: ReLU\n",
    "    pooling_layer,     # 3: MaxPooling\n",
    "    flatten_layer,     # 4: Flatten\n",
    "    flc_layer1,        # 5: FC1\n",
    "    dropout_layer1,    # 6: Dropout1\n",
    "    flc_layer2,        # 7: FC2\n",
    "    dropout_layer2,    # 8: Dropout2\n",
    "    softmax_layer      # 9: Softmax\n",
    "])\n",
    "\n",
    "# Train the model (example with small subset and few epochs for testing)\n",
    "# Uncomment to run:\n",
    "cnn_model.train(train_images[:100], train_labels[:100], epochs=2, batch_size=10, learning_rate=0.01)\n",
    "\n",
    "# Make predictions on test set\n",
    "# predictions = cnn_model.predict(test_images[:10])\n",
    "# print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f5d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
