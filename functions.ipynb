{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "541307d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b4b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps):\n",
    "    # Compute reciprocal of square root of the moving variance elementwise\n",
    "    inv = tf.cast(tf.math.sqrt(moving_var + eps), X.dtype)\n",
    "    # Scale and shift | y = gamma*x - beta\n",
    "    Y = gamma * ((X - moving_mean) / inv) + beta\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b28174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        weight_shape = [input_shape[-1],]\n",
    "\n",
    "        self.gamma = self.add_weight(\n",
    "            name = 'gamma',\n",
    "            shape = weight_shape,\n",
    "            initializer = tf.initializer.ones, \n",
    "            trainable = True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name = 'beta',\n",
    "            shape = weight_shape,\n",
    "            initializer = tf.initializer.zeros, \n",
    "            trainable = True\n",
    "        )\n",
    "        self.moving_mean = self.add_weight(\n",
    "            name='moving_mean',\n",
    "            shape=weight_shape, \n",
    "            initializer=tf.initializers.zeros,\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        self.moving_variance = self.add_weight(\n",
    "            name='moving_variance',\n",
    "            shape=weight_shape, \n",
    "            initializer=tf.initializers.ones,\n",
    "            trainable=False\n",
    "        )\n",
    "        \n",
    "        super(BatchNorm, self).build(input_shape)\n",
    "\n",
    "    def assign_moving_average(self, variable, value):\n",
    "        momentum = 0.1\n",
    "        delta = (1.0 - momentum) * variable + momentum * value\n",
    "        return variable.assign(delta)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training):\n",
    "        if training:\n",
    "            axes = list(range(len(inputs.shape) - 1))\n",
    "            batch_mean = tf.reduce_mean(inputs, axes, keepdims=True)\n",
    "            batch_variance = tf.reduce_mean(tf.math.squared_difference(\n",
    "                inputs, tf.stop_gradient(batch_mean)), axes, keepdims=True)\n",
    "            batch_mean = tf.squeeze(batch_mean, axes)\n",
    "            batch_variance = tf.squeeze(batch_variance, axes)\n",
    "            mean_update = self.assign_moving_average(\n",
    "                self.moving_mean, batch_mean)\n",
    "            variance_update = self.assign_moving_average(\n",
    "                self.moving_variance, batch_variance)\n",
    "            self.add_update(mean_update)\n",
    "            self.add_update(variance_update)\n",
    "            mean, variance = batch_mean, batch_variance\n",
    "        else:\n",
    "            mean, variance = self.moving_mean, self.moving_variance\n",
    "        output = batch_norm(inputs, moving_mean=mean, moving_var=variance,\n",
    "            beta=self.beta, gamma=self.gamma, eps=1e-5)\n",
    "        return output\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c458e59",
   "metadata": {},
   "source": [
    "bn_layer = BatchNorm()\n",
    "x = bn_layer(inputs)\n",
    "--> x = BatchNorm()(inputs)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(64,))       # data: 64 features\n",
    "h = tf.keras.layers.Dense(128)(inputs)     # fully connected layer output\n",
    "x = BatchNorm()(h)   \n",
    "\n",
    "Input shape: (batch_size, n_features)\n",
    "Output shape: (batch_size, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae8b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    # Initialize the dropout layer\n",
    "    def __init__(self, rate):\n",
    "        # 'rate' is the dropout rate (probability to drop a unit)\n",
    "        # Convert it to keep-probability for sampling\n",
    "        # e.g., rate=0.1  -> keep_prob = 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values (sometimes useful for debugging or consistency)\n",
    "        self.inputs = inputs\n",
    "        # Sample a Bernoulli (0/1) mask with keep_prob = self.rate\n",
    "        # and scale by 1/keep_prob (inverted dropout) to keep expected activations unchanged\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        # Apply mask elementwise\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Pass gradients only through the units that were kept in forward\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6297a7",
   "metadata": {},
   "source": [
    "Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flattening (data):\n",
    "    return data.reshape(data.shape[0], data.shape[1]*data.shape[2])\n",
    "#train_images_flat = Flattening(train_images)     \n",
    "#train_images_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820aa56",
   "metadata": {},
   "source": [
    "Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ddccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(batch, probs, labels):\n",
    "    # labels one-hot\n",
    "    loss = -np.sum(labels * np.log(probs + 1e-12)) / batch\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
